import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class SmartCrashAnalyzer:
    def __init__(self):
        print("üß† SMART CRASH ANALYZER –ó–ê–ü–£–©–ï–ù!")
        print("üéØ –£–ú–ù–´–ô –ü–û–î–•–û–î –ö –ê–ù–ê–õ–ò–ó–£ –ö–†–ê–®-–ò–ì–†–´")
        print("üí° –£–ß–ò–¢–´–í–ê–ï–¢ –î–ò–°–ë–ê–õ–ê–ù–° –ö–õ–ê–°–°–û–í –ò –†–ï–ê–õ–¨–ù–û–°–¢–¨")
        print("=" * 60)
    
    def load_and_analyze(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        self.df = pd.read_csv('analytics_dataset_v3_players_41k.csv')
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(self.df):,} –∑–∞–ø–∏—Å–µ–π")
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        hidden_counts = self.df['hidden_state'].value_counts().sort_index()
        print(f"\nüéØ –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:")
        for state, count in hidden_counts.items():
            percent = count / len(self.df) * 100
            print(f"  State {state}: {count:,} ({percent:.3f}%)")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        state_2_count = hidden_counts.get(2, 0)
        if state_2_count < 100:
            print(f"\nüí° –£–ú–ù–ê–Ø –°–¢–†–ê–¢–ï–ì–ò–Ø:")
            print(f"  State 2 —Å–ª–∏—à–∫–æ–º —Ä–µ–¥–∫–∏–π ({state_2_count} –ø—Ä–∏–º–µ—Ä–æ–≤)")
            print(f"  ‚úÖ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –∞–Ω–∞–ª–∏–∑—É State 0 vs State 1")
            print(f"  ‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –ø–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∫—Ä–∞—à–µ–π")
            return "binary_with_anomaly_detection"
        else:
            return "multiclass"
    
    def create_smart_features(self):
        """–°–æ–∑–¥–∞–µ—Ç —É–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ —É—Ç–µ—á–µ–∫"""
        print("\nüîß –°–û–ó–î–ê–ù–ò–ï –£–ú–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í...")
        
        df = self.df.copy()
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        try:
            df['start_at'] = pd.to_datetime(df['start_at'], format='mixed')
        except:
            df['start_at'] = pd.to_datetime(df['start_at'], errors='coerce')
        
        df['hour'] = df['start_at'].dt.hour
        df['day_of_week'] = df['start_at'].dt.dayofweek
        
        # –ß–ï–°–¢–ù–´–ï –ª–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for lag in [1, 2, 3, 5]:
            df[f'crash_lag_{lag}'] = df['crashed_at'].shift(lag)
            df[f'users_lag_{lag}'] = df['users_count'].shift(lag)
            df[f'bank_lag_{lag}'] = df['total_bank_usd'].shift(lag)
        
        # –ß–ï–°–¢–ù–´–ï rolling —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (—Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª—ã–µ –¥–∞–Ω–Ω—ã–µ)
        for window in [5, 10]:
            df[f'crash_mean_{window}'] = df['crashed_at'].shift(1).rolling(window).mean()
            df[f'crash_std_{window}'] = df['crashed_at'].shift(1).rolling(window).std()
            df[f'users_mean_{window}'] = df['users_count'].shift(1).rolling(window).mean()
        
        # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–≥–∏ (expanding, —Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª—ã–µ)
        df['historical_q25'] = df['crashed_at'].shift(1).expanding().quantile(0.25)
        df['historical_q75'] = df['crashed_at'].shift(1).expanding().quantile(0.75)
        df['historical_q90'] = df['crashed_at'].shift(1).expanding().quantile(0.90)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —á–µ—Å—Ç–Ω—ã–µ —Ñ–∏—á–∏
        df['profit_per_user'] = df['profit_loss'] / (df['users_count'] + 1)
        df['bank_per_user'] = df['total_bank_usd'] / (df['users_count'] + 1)
        
        # –£–¥–∞–ª—è–µ–º NaN
        df_clean = df.dropna()
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(df_clean.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, {len(df_clean):,} —Å—Ç—Ä–æ–∫")
        
        return df_clean
    
    def binary_analysis(self, df):
        """–ë–∏–Ω–∞—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ State 0 vs State 1"""
        print("\nüéØ –ë–ò–ù–ê–†–ù–´–ô –ê–ù–ê–õ–ò–ó: State 0 vs State 1")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ State 0 –∏ 1
        binary_df = df[df['hidden_state'].isin([0, 1])].copy()
        print(f"üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(binary_df):,} –∑–∞–ø–∏—Å–µ–π")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        feature_cols = [col for col in binary_df.columns 
                       if col not in ['hidden_state', 'round_id', 'start_at']]
        
        X = binary_df[feature_cols]
        y = binary_df['hidden_state']
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # –ú–æ–¥–µ–ª—å
        rf = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        )
        
        rf.fit(X_train_scaled, y_train)
        
        # –û—Ü–µ–Ω–∫–∞
        y_pred = rf.predict(X_test_scaled)
        y_pred_proba = rf.predict_proba(X_test_scaled)[:, 1]
        
        auc = roc_auc_score(y_test, y_pred_proba)
        
        print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–ò–ù–ê–†–ù–û–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
        print(f"üéØ AUC: {auc:.4f}")
        print(f"\nüìä Classification Report:")
        print(classification_report(y_test, y_pred))
        
        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\nüèÜ –¢–û–ü-10 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
        for i, (idx, row) in enumerate(feature_importance.head(10).iterrows(), 1):
            print(f"  {i:2}. {row['feature']}: {row['importance']:.4f}")
        
        return auc, feature_importance
    
    def anomaly_detection_analysis(self, df):
        """–ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π —Å –ø–æ–º–æ—â—å—é Isolation Forest"""
        print("\nüîç –ê–ù–ê–õ–ò–ó –ê–ù–û–ú–ê–õ–¨–ù–´–• –ö–†–ê–®–ï–ô:")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∫—Ä–∞—à–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π
        crash_features = ['crashed_at', 'total_bank_usd', 'users_count', 'profit_loss']
        X_anomaly = df[crash_features].dropna()
        
        # Isolation Forest
        iso_forest = IsolationForest(
            contamination=0.05,  # 5% –∞–Ω–æ–º–∞–ª–∏–π
            random_state=42,
            n_jobs=-1
        )
        
        anomaly_pred = iso_forest.fit_predict(X_anomaly)
        anomaly_scores = iso_forest.decision_function(X_anomaly)
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π
        anomalies = X_anomaly[anomaly_pred == -1]
        normal = X_anomaly[anomaly_pred == 1]
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(anomalies):,} –∏–∑ {len(X_anomaly):,}")
        print(f"üìä –ü—Ä–æ—Ü–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π: {len(anomalies)/len(X_anomaly)*100:.2f}%")
        
        print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ê–ù–û–ú–ê–õ–¨–ù–´–• –ö–†–ê–®–ï–ô:")
        print(f"  –°—Ä–µ–¥–Ω–∏–π –∫—Ä–∞—à (–Ω–æ—Ä–º–∞): {normal['crashed_at'].mean():.2f}x")
        print(f"  –°—Ä–µ–¥–Ω–∏–π –∫—Ä–∞—à (–∞–Ω–æ–º–∞–ª–∏–∏): {anomalies['crashed_at'].mean():.2f}x")
        print(f"  –ú–µ–¥–∏–∞–Ω–∞ (–Ω–æ—Ä–º–∞): {normal['crashed_at'].median():.2f}x")
        print(f"  –ú–µ–¥–∏–∞–Ω–∞ (–∞–Ω–æ–º–∞–ª–∏–∏): {anomalies['crashed_at'].median():.2f}x")
        
        # –¢–æ–ø –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∫—Ä–∞—à–µ–π
        anomaly_df = df.iloc[X_anomaly.index[anomaly_pred == -1]].copy()
        anomaly_df['anomaly_score'] = anomaly_scores[anomaly_pred == -1]
        top_anomalies = anomaly_df.nsmallest(10, 'anomaly_score')
        
        print(f"\nüèÜ –¢–û–ü-10 –°–ê–ú–´–• –ê–ù–û–ú–ê–õ–¨–ù–´–• –†–ê–£–ù–î–û–í:")
        for i, (idx, row) in enumerate(top_anomalies.iterrows(), 1):
            print(f"  {i:2}. Round {row['round_id']}: {row['crashed_at']:.2f}x "
                  f"(score: {row['anomaly_score']:.3f})")
        
        return anomalies, normal, top_anomalies
    
    def crash_pattern_analysis(self, df):
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∫—Ä–∞—à–µ–π"""
        print("\nüìä –ê–ù–ê–õ–ò–ó –ü–ê–¢–¢–ï–†–ù–û–í –ö–†–ê–®–ï–ô:")
        
        crashes = df['crashed_at']
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print(f"üìà –û–°–ù–û–í–ù–´–ï –°–¢–ê–¢–ò–°–¢–ò–ö–ò:")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ: {crashes.mean():.2f}x")
        print(f"  –ú–µ–¥–∏–∞–Ω–∞: {crashes.median():.2f}x")
        print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {crashes.std():.2f}x")
        print(f"  –ú–∏–Ω–∏–º—É–º: {crashes.min():.2f}x")
        print(f"  –ú–∞–∫—Å–∏–º—É–º: {crashes.max():.2f}x")
        
        # –ö–≤–∞–Ω—Ç–∏–ª–∏
        quantiles = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]
        print(f"\nüìä –ö–í–ê–ù–¢–ò–õ–ò:")
        for q in quantiles:
            value = crashes.quantile(q)
            print(f"  Q{q*100:2.0f}: {value:.2f}x")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º
        bins = [1.0, 1.5, 2.0, 3.0, 5.0, 10.0, 25.0, float('inf')]
        labels = ['1.0-1.5', '1.5-2.0', '2.0-3.0', '3.0-5.0', '5.0-10.0', '10.0-25.0', '25.0+']
        
        crash_ranges = pd.cut(crashes, bins=bins, labels=labels, right=False)
        range_counts = crash_ranges.value_counts().sort_index()
        
        print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –î–ò–ê–ü–ê–ó–û–ù–ê–ú:")
        for range_name, count in range_counts.items():
            percent = count / len(crashes) * 100
            print(f"  {range_name}: {count:,} ({percent:.1f}%)")
    
    def run_complete_analysis(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —É–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        print("üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –£–ú–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê...")
        
        # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        strategy = self.load_and_analyze()
        
        # 2. –°–æ–∑–¥–∞–µ–º —É–º–Ω—ã–µ —Ñ–∏—á–∏
        df_features = self.create_smart_features()
        
        # 3. –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∫—Ä–∞—à–µ–π
        self.crash_pattern_analysis(df_features)
        
        # 4. –ë–∏–Ω–∞—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        auc, feature_importance = self.binary_analysis(df_features)
        
        # 5. –ü–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª–∏–π
        anomalies, normal, top_anomalies = self.anomaly_detection_analysis(df_features)
        
        # 6. –ò—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã
        print(f"\nüéä –ò–¢–û–ì–û–í–´–ï –í–´–í–û–î–´:")
        print(f"üìä –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è AUC: {auc:.4f}")
        if auc < 0.55:
            print(f"  ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –±–ª–∏–∑–æ–∫ –∫ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ - –∏–≥—Ä–∞ —á–µ—Å—Ç–Ω–∞—è!")
        elif auc < 0.65:
            print(f"  ü§î –°–ª–∞–±–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å - –≤–æ–∑–º–æ–∂–Ω—ã —Å–∫—Ä—ã—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
        else:
            print(f"  ‚ö†Ô∏è  –í—ã—Å–æ–∫–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö!")
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∫—Ä–∞—à–µ–π: {len(anomalies):,}")
        print(f"üìà –°—Ä–µ–¥–Ω–∏–π –∞–Ω–æ–º–∞–ª—å–Ω—ã–π –∫—Ä–∞—à: {anomalies['crashed_at'].mean():.2f}x")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        feature_importance.to_csv(f'smart_analysis_features_{timestamp}.csv', index=False)
        top_anomalies.to_csv(f'top_anomalies_{timestamp}.csv', index=False)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Å timestamp: {timestamp}")
        
        return {
            'auc': auc,
            'anomalies_count': len(anomalies),
            'feature_importance': feature_importance,
            'top_anomalies': top_anomalies
        }

def main():
    print("üß†" * 30)
    print("üß† SMART CRASH ANALYZER")
    print("üß†" * 30)
    print("üí° –û–°–û–ë–ï–ù–ù–û–°–¢–ò:")
    print("  ‚úÖ –£—á–∏—Ç—ã–≤–∞–µ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤")
    print("  ‚úÖ –£–º–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∞–Ω–∞–ª–∏–∑–∞")
    print("  ‚úÖ –ß–µ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –±–µ–∑ —É—Ç–µ—á–µ–∫")
    print("  ‚úÖ –ü–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∫—Ä–∞—à–µ–π")
    print("  ‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
    print("  ‚úÖ –ü—Ä–∞–∫—Ç–∏—á–Ω—ã–µ –≤—ã–≤–æ–¥—ã")
    print("=" * 60)
    
    analyzer = SmartCrashAnalyzer()
    results = analyzer.run_complete_analysis()
    
    print("\nüéØ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
    return results

if __name__ == "__main__":
    main()
