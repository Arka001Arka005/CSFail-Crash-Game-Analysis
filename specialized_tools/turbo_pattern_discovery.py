"""
üî•üî•üî• TURBO PATTERN DISCOVERY üî•üî•üî•
–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –°–ö–û–†–û–°–¢–ò!

–ö–ª—é—á–µ–≤—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
- –ú–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ—Å—Ç—å –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä TSFresh –ø—Ä–∏–∑–Ω–∞–∫–æ–≤  
- –†–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–Ω–∏–µ FeatureTools
- –ë—ã—Å—Ç—Ä—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# –ú–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ—Å—Ç—å
import multiprocessing as mp
from joblib import Parallel, delayed

# Auto Feature Engineering (–ë–´–°–¢–†–´–ï)
try:
    import featuretools as ft
    FEATURETOOLS_AVAILABLE = True
except ImportError:
    FEATURETOOLS_AVAILABLE = False
    print("‚ö†Ô∏è FeatureTools –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# Time Series Features (–û–ì–†–ê–ù–ò–ß–ï–ù–ù–´–ï)
try:
    from tsfresh import extract_features
    from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters
    TSFRESH_AVAILABLE = True
except ImportError:
    TSFRESH_AVAILABLE = False
    print("‚ö†Ô∏è TSFresh –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# Fast AutoML
try:
    from tpot import TPOTClassifier
    TPOT_AVAILABLE = True
except ImportError:
    TPOT_AVAILABLE = False
    print("‚ö†Ô∏è TPOT –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# FAST Pattern Mining
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score
import itertools
from datetime import datetime
from tqdm import tqdm
import time
import os

class TurboPatternDiscovery:
    def __init__(self, data_directory=".", target_column="hidden_state", max_workers=None):
        """
        üöÄ –¢–£–†–ë–û –ü–û–ò–°–ö –ü–ê–¢–¢–ï–†–ù–û–í –° –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –°–ö–û–†–û–°–¢–¨–Æ!
        
        max_workers: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ (None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        """
        self.data_directory = Path(data_directory)
        self.target_column = target_column
        self.combined_df = None
        
        # –ú–ù–û–ì–û–ü–†–û–¶–ï–°–°–û–†–ù–û–°–¢–¨
        if max_workers is None:
            self.max_workers = mp.cpu_count()
        else:
            self.max_workers = max_workers
            
        print(f"üî• TURBO MODE: {self.max_workers} –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
        self.check_libraries()
    
    def check_libraries(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        print(f"\nüì¶ –¢–£–†–ë–û –ü–†–û–í–ï–†–ö–ê –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í:")
        print(f"  FeatureTools (Auto FE): {'‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω' if FEATURETOOLS_AVAILABLE else '‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω'}")
        print(f"  TSFresh (Time Series): {'‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω' if TSFRESH_AVAILABLE else '‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω'}")
        print(f"  TPOT (AutoML): {'‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω' if TPOT_AVAILABLE else '‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω'}")
        print(f"  –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤: {self.max_workers} üî•")
    
    def discover_all_csv_files(self):
        """–ë–´–°–¢–†–û –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ CSV —Ñ–∞–π–ª—ã"""
        csv_files = list(self.data_directory.glob("*.csv"))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É (—Å–Ω–∞—á–∞–ª–∞ –º–∞–ª–µ–Ω—å–∫–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞)
        csv_files_with_size = []
        for csv_file in csv_files:
            size_mb = csv_file.stat().st_size / (1024 * 1024)
            csv_files_with_size.append((csv_file, size_mb))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É (–º–∞–ª–µ–Ω—å–∫–∏–µ —Å–Ω–∞—á–∞–ª–∞)
        csv_files_with_size.sort(key=lambda x: x[1])
        
        print(f"\nüìä –ù–ê–ô–î–ï–ù–û CSV –§–ê–ô–õ–û–í: {len(csv_files_with_size)}")
        for csv_file, size_mb in csv_files_with_size:
            print(f"  üìÑ {csv_file.name} ({size_mb:.1f} MB)")
        
        return [csv_file for csv_file, _ in csv_files_with_size]
    
    def fast_load_csv(self, csv_path):
        """–ë–´–°–¢–†–ê–Ø –∑–∞–≥—Ä—É–∑–∫–∞ CSV —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏"""
        try:
            # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ - —Ç–æ–ª—å–∫–æ sample
            file_size_mb = csv_path.stat().st_size / (1024 * 1024)
            
            if file_size_mb > 50:  # –ë–æ–ª—å—à–µ 50MB
                print(f"  üìä {csv_path.name}: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ 20k —Å—Ç—Ä–æ–∫")
                df = pd.read_csv(csv_path, nrows=20000, low_memory=False)
            elif file_size_mb > 10:  # –ë–æ–ª—å—à–µ 10MB
                print(f"  üìä {csv_path.name}: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ 50k —Å—Ç—Ä–æ–∫")
                df = pd.read_csv(csv_path, nrows=50000, low_memory=False)
            else:
                df = pd.read_csv(csv_path, low_memory=False)
                print(f"  ‚úÖ {csv_path.name}: {len(df):,} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
            
            return csv_path.stem, df
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {csv_path.name}: {e}")
            return csv_path.stem, None
    
    def load_and_analyze_datasets(self, csv_files):
        """–ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        print(f"\nüîç –¢–£–†–ë–û –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –î–ê–¢–ê–°–ï–¢–û–í...")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        results = Parallel(n_jobs=min(4, len(csv_files)), prefer="threads")(
            delayed(self.fast_load_csv)(csv_file) for csv_file in tqdm(csv_files, desc="–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤")
        )
        
        datasets = {}
        for name, df in results:
            if df is not None:
                datasets[name] = df
        
        return datasets
    
    def create_turbo_basic_features(self, datasets):
        """–ë–´–°–¢–†–û–ï —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print(f"\nüîß –¢–£–†–ë–û –°–û–ó–î–ê–ù–ò–ï –ë–ê–ó–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í...")
        
        # –ù–∞—Ö–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç (—Å–∞–º—ã–π –±–æ–ª—å—à–æ–π —Å target_column)
        main_dataset = None
        main_name = None
        
        for name, df in datasets.items():
            if self.target_column in df.columns:
                if main_dataset is None or len(df) > len(main_dataset):
                    main_dataset = df.copy()
                    main_name = name
        
        if main_dataset is None:
            print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–æ–π '{self.target_column}'")
            return None
        
        print(f"üìä –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç: {main_name} ({len(main_dataset):,} —Å—Ç—Ä–æ–∫)")
        
        # –ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        numeric_cols = main_dataset.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col != self.target_column]
        
        print(f"üî¢ –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è {len(numeric_cols)} —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫...")
        
        for col in tqdm(numeric_cols, desc="–ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"):
            if col in main_dataset.columns:
                # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ)
                main_dataset[f'{col}_lag_1'] = main_dataset[col].shift(1)
                main_dataset[f'{col}_lag_3'] = main_dataset[col].shift(3)
                main_dataset[f'{col}_lag_5'] = main_dataset[col].shift(5)
                
                # Rolling –ø—Ä–∏–∑–Ω–∞–∫–∏ (–±—ã—Å—Ç—Ä—ã–µ)
                main_dataset[f'{col}_roll_mean_5'] = main_dataset[col].rolling(5, min_periods=1).mean()
                main_dataset[f'{col}_roll_std_5'] = main_dataset[col].rolling(5, min_periods=1).std()
                main_dataset[f'{col}_roll_max_10'] = main_dataset[col].rolling(10, min_periods=1).max()
                main_dataset[f'{col}_roll_min_10'] = main_dataset[col].rolling(10, min_periods=1).min()
                
                # Expanding –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤–∞–∂–Ω—ã–µ)
                main_dataset[f'{col}_expanding_mean'] = main_dataset[col].expanding(min_periods=1).mean()
                main_dataset[f'{col}_expanding_std'] = main_dataset[col].expanding(min_periods=1).std()
                
                # –ö–≤–∞–Ω—Ç–∏–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ (–±–µ–∑ —É—Ç–µ—á–µ–∫!)
                main_dataset[f'{col}_above_q75'] = (main_dataset[col] > main_dataset[col].shift(1).expanding().quantile(0.75)).astype(int)
                main_dataset[f'{col}_above_q90'] = (main_dataset[col] > main_dataset[col].shift(1).expanding().quantile(0.90)).astype(int)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –¥—Ä—É–≥–∏–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ (—Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ)
        for name, df in datasets.items():
            if name != main_name and 'round_id' in df.columns and 'round_id' in main_dataset.columns:
                print(f"üîó –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å {name}...")
                
                # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ round_id (–±—ã—Å—Ç—Ä–æ)
                numeric_cols_other = df.select_dtypes(include=[np.number]).columns
                numeric_cols_other = [col for col in numeric_cols_other if col not in ['round_id']]
                
                if len(numeric_cols_other) > 0:
                    agg_df = df.groupby('round_id')[numeric_cols_other].agg(['mean', 'std', 'count']).fillna(0)
                    agg_df.columns = [f'{name}_{col}_{stat}' for col, stat in agg_df.columns]
                    agg_df = agg_df.reset_index()
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º
                    main_dataset = main_dataset.merge(agg_df, on='round_id', how='left')
        
        print(f"‚úÖ –ë–ê–ó–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò –°–û–ó–î–ê–ù–´: {len(main_dataset.columns)} –∫–æ–ª–æ–Ω–æ–∫")
        
        return main_dataset
    
    def create_turbo_interaction_features(self, df, max_interactions=30):
        """–ë–´–°–¢–†–´–ï –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è"""
        print(f"\nüîÑ –¢–£–†–ë–û –°–û–ó–î–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø...")
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col != self.target_column]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (–ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å target)
        if len(numeric_cols) > 0 and self.target_column in df.columns:
            correlations = df[numeric_cols + [self.target_column]].corr()[self.target_column].abs().sort_values(ascending=False)
            top_cols = correlations.head(10).index.tolist()
            top_cols = [col for col in top_cols if col != self.target_column]
        else:
            top_cols = numeric_cols[:10]  # –ü–µ—Ä–≤—ã–µ 10
        
        print(f"üî¢ –°–æ–∑–¥–∞–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —Ç–æ–ø-{len(top_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        interaction_count = 0
        for i, col1 in enumerate(top_cols):
            if interaction_count >= max_interactions:
                break
                
            for j, col2 in enumerate(top_cols[i+1:], i+1):
                if interaction_count >= max_interactions:
                    break
                
                # –¢–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
                df[f'{col1}_plus_{col2}'] = df[col1] + df[col2]
                df[f'{col1}_mult_{col2}'] = df[col1] * df[col2]
                
                # –î–µ–ª–µ–Ω–∏–µ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –Ω—É–ª—è
                df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-8)
                
                interaction_count += 3
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {interaction_count} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è")
        
        return df
    
    def create_turbo_tsfresh_features(self, df):
        """–ë–´–°–¢–†–´–ï TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        if not TSFRESH_AVAILABLE:
            print("‚ö†Ô∏è TSFresh –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return df
        
        print(f"\n‚è∞ –¢–£–†–ë–û TSFresh –ü–†–ò–ó–ù–ê–ö–ò...")
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è TSFresh
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        target_cols = [col for col in numeric_cols if 'crashed_at' in col or 'cashout' in col or 'pnl' in col]
        target_cols = target_cols[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –∫–æ–ª–æ–Ω–∫–∏
        
        if len(target_cols) == 0:
            print("‚ö†Ô∏è –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è TSFresh")
            return df
        
        print(f"üìä TSFresh –¥–ª—è {len(target_cols)} –∫–æ–ª–æ–Ω–æ–∫: {target_cols}")
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è TSFresh
            ts_data = df[['round_id'] + target_cols].copy()
            ts_data = ts_data.dropna()
            
            if len(ts_data) < 100:
                print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è TSFresh")
                return df
            
            # –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            extraction_settings = MinimalFCParameters()  # –°–∞–º—ã–µ –±—ã—Å—Ç—Ä—ã–µ
            
            # TSFresh —Å –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ—Å—Ç—å—é
            tsfresh_features = extract_features(
                ts_data.drop('round_id', axis=1), 
                column_id=None,
                default_fc_parameters=extraction_settings,
                n_jobs=self.max_workers,  # –ú–ù–û–ì–û –ü–†–û–¶–ï–°–°–û–†–û–í!
                show_warnings=False
            )
            
            print(f"‚úÖ TSFresh —Å–æ–∑–¥–∞–ª {len(tsfresh_features.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
            tsfresh_features = tsfresh_features.reset_index(drop=True)
            for col in tsfresh_features.columns:
                if col not in df.columns:
                    df[f'tsfresh_{col}'] = tsfresh_features[col].fillna(0)
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ TSFresh: {e}")
        
        return df
    
    def turbo_feature_selection(self, df):
        """–ë–´–°–¢–†–´–ô –æ—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print(f"\nüèÜ –¢–£–†–ë–û –û–¢–ë–û–† –õ–£–ß–®–ò–• –ü–†–ò–ó–ù–ê–ö–û–í...")
        
        if self.target_column not in df.columns:
            print(f"‚ùå –¶–µ–ª–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ '{self.target_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return df, []
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        feature_columns = [col for col in df.columns if col != self.target_column]
        X = df[feature_columns].select_dtypes(include=[np.number])
        y = df[self.target_column]
        
        # –£–±–∏—Ä–∞–µ–º NaN
        mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[mask].fillna(0)
        y = y[mask]
        
        if len(X) < 100:
            print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return df, list(X.columns)
        
        print(f"üìä –û—Ç–±–∏—Ä–∞–µ–º –∏–∑ {len(X.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        selected_features = []
        
        # 1. SelectKBest (–±—ã—Å—Ç—Ä–æ!)
        try:
            selector = SelectKBest(score_func=f_classif, k=min(50, len(X.columns)))
            X_selected = selector.fit_transform(X, y)
            selected_mask = selector.get_support()
            kbest_features = X.columns[selected_mask].tolist()
            selected_features.extend(kbest_features)
            print(f"  ‚úÖ SelectKBest: {len(kbest_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        except:
            print("  ‚ö†Ô∏è SelectKBest –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª")
        
        # 2. Random Forest importance (–±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å)
        try:
            rf = RandomForestClassifier(n_estimators=50, max_depth=10, n_jobs=self.max_workers, random_state=42)
            rf.fit(X.fillna(0), y)
            
            # –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            feature_importance = pd.DataFrame({
                'feature': X.columns,
                'importance': rf.feature_importances_
            }).sort_values('importance', ascending=False)
            
            rf_features = feature_importance.head(50)['feature'].tolist()
            selected_features.extend(rf_features)
            print(f"  ‚úÖ Random Forest: —Ç–æ–ø {len(rf_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Random Forest –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        final_features = list(set(selected_features))
        final_features = [col for col in final_features if col in df.columns]
        
        print(f"‚úÖ –ò–¢–û–ì–û –õ–£–ß–®–ò–• –ü–†–ò–ó–ù–ê–ö–û–í: {len(final_features)}")
        
        return df, final_features
    
    def quick_automl_test(self, df, selected_features):
        """–ë–´–°–¢–†–´–ô —Ç–µ—Å—Ç AutoML"""
        if not TPOT_AVAILABLE:
            print("‚ö†Ô∏è TPOT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è AutoML")
            return
        
        if self.target_column not in df.columns:
            print("‚ùå –ù–µ—Ç —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è AutoML")
            return
        
        print(f"\nü§ñ –ë–´–°–¢–†–´–ô AUTOML –¢–ï–°–¢...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        feature_columns = selected_features if selected_features else [col for col in df.columns if col != self.target_column and df[col].dtype in ['int64', 'float64']]
        feature_columns = feature_columns[:100]  # –ú–∞–∫—Å–∏–º—É–º 100 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        
        X = df[feature_columns].fillna(0)
        y = df[self.target_column].fillna(0)
        
        # –£–±–∏—Ä–∞–µ–º NaN
        mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[mask]
        y = y[mask]
        
        if len(X) < 100:
            print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è AutoML")
            return
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        try:
            # –ë–´–°–¢–†–´–ô TPOT (–º–∞–ª–æ –ø–æ–∫–æ–ª–µ–Ω–∏–π)
            tpot = TPOTClassifier(
                generations=3,  # –ú–∞–ª–æ –ø–æ–∫–æ–ª–µ–Ω–∏–π –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                population_size=20,  # –ú–∞–ª–µ–Ω—å–∫–∞—è –ø–æ–ø—É–ª—è—Ü–∏—è
                cv=3,  # –ë—ã—Å—Ç—Ä–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
                scoring='roc_auc',
                n_jobs=self.max_workers,  # –í–°–ï –ü–†–û–¶–ï–°–°–û–†–´!
                random_state=42,
                verbosity=2,
                max_time_mins=10  # –ú–∞–∫—Å–∏–º—É–º 10 –º–∏–Ω—É—Ç
            )
            
            print(f"üöÄ –ó–∞–ø—É—Å–∫ TURBO AutoML –Ω–∞ {len(X_train)} –æ–±—Ä–∞–∑—Ü–∞—Ö...")
            tpot.fit(X_train, y_train)
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º
            score = tpot.score(X_test, y_test)
            print(f"‚úÖ AUTOML –†–ï–ó–£–õ–¨–¢–ê–¢: {score:.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            tpot.export(f'turbo_tpot_best_pipeline_{timestamp}.py')
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å: turbo_tpot_best_pipeline_{timestamp}.py")
            
        except Exception as e:
            print(f"‚ö†Ô∏è AutoML –æ—à–∏–±–∫–∞: {e}")
    
    def run_turbo_discovery(self):
        """üî• –ì–õ–ê–í–ù–´–ô –¢–£–†–ë–û –ü–û–ò–°–ö –ü–ê–¢–¢–ï–†–ù–û–í! üî•"""
        
        print(f"""
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
üî• TURBO PATTERN DISCOVERY - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –°–ö–û–†–û–°–¢–¨!
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
üéØ –¢–£–†–ë–û –û–°–û–ë–ï–ù–ù–û–°–¢–ò:
  ‚ö° {self.max_workers} –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ —Ä–∞–±–æ—Ç–∞—é—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ!
  ‚ö° –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
  ‚ö° –ë—ã—Å—Ç—Ä—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  ‚ö° –£—Å–∫–æ—Ä–µ–Ω–Ω—ã–π AutoML (3 –ø–æ–∫–æ–ª–µ–Ω–∏—è, 10 –º–∏–Ω—É—Ç –º–∞–∫—Å)
  ‚ö° –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
============================================================
üöÄ –¢–£–†–ë–û –ü–û–ò–°–ö –ó–ê–ü–£–©–ï–ù!
============================================================
        """)
        
        start_time = time.time()
        
        # 1. –ù–∞—Ö–æ–¥–∏–º CSV —Ñ–∞–π–ª—ã
        csv_files = self.discover_all_csv_files()
        if not csv_files:
            print("‚ùå CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            return
        
        # 2. –ë–´–°–¢–†–û –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        datasets = self.load_and_analyze_datasets(csv_files)
        if not datasets:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
            return
        
        # 3. –¢–£–†–ë–û —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        combined_df = self.create_turbo_basic_features(datasets)
        if combined_df is None:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏!")
            return
        
        # 4. –ü—Ä–∏–∑–Ω–∞–∫–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        combined_df = self.create_turbo_interaction_features(combined_df)
        
        # 5. TSFresh (–±—ã—Å—Ç—Ä–æ!)
        combined_df = self.create_turbo_tsfresh_features(combined_df)
        
        print(f"\nüìä –û–ë–©–ò–ï –ü–†–ò–ó–ù–ê–ö–ò –°–û–ó–î–ê–ù–´: {len(combined_df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
        
        # 6. –¢–£–†–ë–û –æ—Ç–±–æ—Ä –ª—É—á—à–∏—Ö
        combined_df, selected_features = self.turbo_feature_selection(combined_df)
        
        # 7. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        output_file = f'turbo_all_features_{timestamp}.csv'
        combined_df.to_csv(output_file, index=False)
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {output_file}")
        
        # –õ—É—á—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        best_features_file = f'turbo_best_features_{timestamp}.txt'
        with open(best_features_file, 'w', encoding='utf-8') as f:
            f.write("üî• TURBO BEST FEATURES üî•\n")
            f.write(f"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(combined_df.columns)}\n")
            f.write(f"–õ—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(selected_features)}\n\n")
            for i, feature in enumerate(selected_features, 1):
                f.write(f"{i:3d}. {feature}\n")
        
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ª—É—á—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {best_features_file}")
        
        # 8. –ë–´–°–¢–†–´–ô AutoML
        self.quick_automl_test(combined_df, selected_features)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"""
====================================================================
üéä TURBO PATTERN DISCOVERY –ó–ê–í–ï–†–®–ï–ù! üéä
====================================================================
‚ö° –°–ö–û–†–û–°–¢–¨: {total_time/60:.1f} –º–∏–Ω—É—Ç (–≤–º–µ—Å—Ç–æ —á–∞—Å–æ–≤!)
üìä –°–û–ó–î–ê–ù–û –ü–†–ò–ó–ù–ê–ö–û–í: {len(combined_df.columns):,}
üèÜ –õ–£–ß–®–ò–• –ü–†–ò–ó–ù–ê–ö–û–í: {len(selected_features)}
ü§ñ AUTOML: –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω
üíæ –§–ê–ô–õ–´: {output_file}, {best_features_file}
====================================================================
üî• –¢–ï–ü–ï–†–¨ –ê–ù–ê–õ–ò–ó –†–ê–ë–û–¢–ê–ï–¢ –í {self.max_workers}x –ë–´–°–¢–†–ï–ï! üî•
====================================================================
        """)


if __name__ == "__main__":
    # üî• –¢–£–†–ë–û –ó–ê–ü–£–°–ö!
    analyzer = TurboPatternDiscovery(max_workers=None)  # –í—Å–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã!
    analyzer.run_turbo_discovery()
